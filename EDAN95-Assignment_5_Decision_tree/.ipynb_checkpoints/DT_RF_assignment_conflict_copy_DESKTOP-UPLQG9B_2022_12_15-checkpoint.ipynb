{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5\n",
    "## Decision Trees and Random Forests for Regression\n",
    "\n",
    "### About this notebook\n",
    "\n",
    "The notebook has two parts, as has the assignment, the first part being a \"tutorial like\" walkthrough, the second asking you to implement a regression tree yourself. Hence, when going through the first part, remember that you need to have time left to actually work with the second!\n",
    "\n",
    "Both parts of the assignment can be handled within this notebook, however, the ID3-implementation should be done in a separate Python-script (ID3_reg.py). Feel free to move the second part to its own notebook for better overview. In any case, if you make changes in the Python file, you might need to restart the kernel for the notebook that is using it, so that changes get properly loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (22.2.2)\n",
      "Collecting pip\n",
      "  Using cached pip-22.3.1-py3-none-any.whl (2.1 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\sasyn\\anaconda3\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: dtreeviz in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (1.4.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (from dtreeviz) (3.5.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (from dtreeviz) (1.4.4)\n",
      "Requirement already satisfied: graphviz>=0.9 in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (from dtreeviz) (0.20.1)\n",
      "Requirement already satisfied: colour in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (from dtreeviz) (0.1.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (from dtreeviz) (1.21.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (from dtreeviz) (1.0.2)\n",
      "Requirement already satisfied: pytest in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (from dtreeviz) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\sasyn\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->dtreeviz) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (from matplotlib->dtreeviz) (4.25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (from matplotlib->dtreeviz) (9.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sasyn\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->dtreeviz) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sasyn\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->dtreeviz) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (from matplotlib->dtreeviz) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (from matplotlib->dtreeviz) (1.4.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (from pandas->dtreeviz) (2022.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (from pytest->dtreeviz) (21.4.0)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (from pytest->dtreeviz) (1.1.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (from pytest->dtreeviz) (1.0.0)\n",
      "Requirement already satisfied: py>=1.8.2 in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (from pytest->dtreeviz) (1.11.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (from pytest->dtreeviz) (2.0.1)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (from pytest->dtreeviz) (1.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sasyn\\appdata\\roaming\\python\\python39\\site-packages (from pytest->dtreeviz) (0.4.6)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (from scikit-learn->dtreeviz) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (from scikit-learn->dtreeviz) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (from scikit-learn->dtreeviz) (1.9.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sasyn\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib->dtreeviz) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (1.21.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\sasyn\\anaconda3\\lib\\site-packages (1.9.1)\n"
     ]
    }
   ],
   "source": [
    "# YOU DON'T HAVE TO RUN THIS IF EVERYTHING IS ALREADY INSTALLED CORRECTLY\n",
    "!pip3 install --upgrade pip\n",
    "!pip3 install graphviz\n",
    "!pip3 install dtreeviz\n",
    "!pip3 install numpy scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the dataset. Ultimately, you should be working with the California housing data, but for quicker test runs, it might help to first start out with the Diabetes data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run time 0.8s\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from ConceptDataRegr import ConceptDataRegr\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "test_case = 'diabetes'\n",
    "#test_case = 'california'\n",
    "\n",
    "if test_case == 'california':\n",
    "    dataset = fetch_california_housing()\n",
    "elif test_case == 'diabetes':\n",
    "    dataset = load_diabetes()\n",
    "else:\n",
    "    raise ValueError('Unknown test case')\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some information about the dataset you're looking at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
      "description: .. _diabetes_dataset:\n",
      "\n",
      "Diabetes dataset\n",
      "----------------\n",
      "\n",
      "Ten baseline variables, age, sex, body mass index, average blood\n",
      "pressure, and six blood serum measurements were obtained for each of n =\n",
      "442 diabetes patients, as well as the response of interest, a\n",
      "quantitative measure of disease progression one year after baseline.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "  :Number of Instances: 442\n",
      "\n",
      "  :Number of Attributes: First 10 columns are numeric predictive values\n",
      "\n",
      "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
      "\n",
      "  :Attribute Information:\n",
      "      - age     age in years\n",
      "      - sex\n",
      "      - bmi     body mass index\n",
      "      - bp      average blood pressure\n",
      "      - s1      tc, total serum cholesterol\n",
      "      - s2      ldl, low-density lipoproteins\n",
      "      - s3      hdl, high-density lipoproteins\n",
      "      - s4      tch, total cholesterol / HDL\n",
      "      - s5      ltg, possibly log of serum triglycerides level\n",
      "      - s6      glu, blood sugar level\n",
      "\n",
      "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n",
      "\n",
      "Source URL:\n",
      "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
      "\n",
      "For more information see:\n",
      "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
      "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n"
     ]
    }
   ],
   "source": [
    "if test_case == 'california' :\n",
    "    print(\"target:\", list(dataset.target_names))\n",
    "print(\"features:\", list(dataset.feature_names))\n",
    "print(\"description:\", dataset.DESCR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split it into train, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run time 0.7s\n",
    "\n",
    "train_ratio = 0.70\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "X = dataset.data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1 - train_ratio, random_state=0)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio), random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the reason why you would like to have validation and test set and not just a test set?\n",
    "\n",
    "More guiding questions:\n",
    "How can you use the validation set?\n",
    "Can you use the validation set in some way to tune the hyperparameters of your model?\n",
    "Can you use the test set to tune the hyperparameters of your model?\n",
    "You can come back to these questions later if you're not sure yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor\n",
    "\n",
    "Run the cells below and inspect the output. Use the documentation where needed. Be prepared to answer questions posed by the TA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run time 0.7s\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regressor1 = DecisionTreeRegressor(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's examine the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02144221,  0.0614295 ,  0.09449354, -0.05130063,  0.36838284,\n",
       "       -0.55354206, -0.00338336, -0.14397523,  0.25445472,  0.12892033])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run time 1.8s\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(regressor1, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.057750608640022794"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run time 0.3s\n",
    "\n",
    "regressor1.fit(X_train, y_train)\n",
    "regressor1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these numbers mean?\n",
    "Check out [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n",
    "and [DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html?highlight=decision+tree)\n",
    "to find the answers.\n",
    "\n",
    "How do you interpret the numbers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at two other parameters, max_depth and min_samples_leaf.\n",
    "How do you interpret the following numbers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.28990619, 0.57340267, 0.30563286, 0.30554873, 0.51034927,\n",
       "       0.38155873, 0.4567179 , 0.4764957 , 0.33169721, 0.43266377])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run time 0.2s\n",
    "\n",
    "regressor2 = DecisionTreeRegressor(max_depth=2, random_state=0)\n",
    "cross_val_score(regressor2, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06463994430394626"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run time 0.1s\n",
    "\n",
    "regressor2.fit(X_train, y_train)\n",
    "regressor2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.22997051, 0.47181244, 0.19624367, 0.0331379 , 0.50615802,\n",
       "       0.16894557, 0.41027993, 0.4764957 , 0.3984747 , 0.39684228])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run time 1.2s\n",
    "\n",
    "regressor3 = DecisionTreeRegressor(min_samples_leaf=50, random_state=0)\n",
    "cross_val_score(regressor3, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19103801072442916"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run time 0.1s\n",
    "\n",
    "regressor3.fit(X_train, y_train)\n",
    "regressor3.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cells give examples how to visualize regressor2 and regressor3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ENGINES' from 'graphviz.backend' (C:\\Users\\sasyn\\anaconda3\\lib\\site-packages\\graphviz\\backend\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#run time 0.2s\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      7\u001b[0m dot_data \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mexport_graphviz(regressor2, feature_names\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mfeature_names, out_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, filled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rounded\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, special_characters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\graphviz\\__init__.py:30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfiles\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Source\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nohtml\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (render, pipe, version, view,\n\u001b[0;32m     31\u001b[0m                       ENGINES, FORMATS, RENDERERS, FORMATTERS,\n\u001b[0;32m     32\u001b[0m                       ExecutableNotFound, RequiredArgumentError)\n\u001b[0;32m     34\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGraph\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDigraph\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExecutableNotFound\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRequiredArgumentError\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     41\u001b[0m ]\n\u001b[0;32m     43\u001b[0m __title__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraphviz\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ENGINES' from 'graphviz.backend' (C:\\Users\\sasyn\\anaconda3\\lib\\site-packages\\graphviz\\backend\\__init__.py)"
     ]
    }
   ],
   "source": [
    "#run time 0.2s\n",
    "\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "from IPython.display import Image\n",
    "\n",
    "dot_data = tree.export_graphviz(regressor2, feature_names=dataset.feature_names, out_file=None, filled=True, rounded=True, special_characters=True)\n",
    "graph = graphviz.Source(dot_data, format=\"png\") \n",
    "graph.render(\"decision_tree_regressor2\")\n",
    "Image(\"decision_tree_regressor2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graphviz' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#run time 4.8s\u001b[39;00m\n\u001b[0;32m      3\u001b[0m dot_data \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mexport_graphviz(regressor3, feature_names\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mfeature_names, out_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, filled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rounded\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, special_characters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mgraphviz\u001b[49m\u001b[38;5;241m.\u001b[39mSource(dot_data, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m      5\u001b[0m graph\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecision_tree_regressor3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m Image(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecision_tree_regressor3.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'graphviz' is not defined"
     ]
    }
   ],
   "source": [
    "#run time 4.8s\n",
    "\n",
    "dot_data = tree.export_graphviz(regressor3, feature_names=dataset.feature_names, out_file=None, filled=True, rounded=True, special_characters=True)\n",
    "graph = graphviz.Source(dot_data, format=\"png\") \n",
    "graph.render(\"decision_tree_regressor3\")\n",
    "Image(\"decision_tree_regressor3.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another nice way to visualize the decision trees nicely is with dtreeviz. To make these plots it takes quite some time, so we recommend to use this visualization tool for trees with few nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 6.9s\n",
    "\n",
    "from dtreeviz.trees import dtreeviz\n",
    "\n",
    "viz = dtreeviz(regressor2, X, y,\n",
    "                target_name=\"target\",\n",
    "                feature_names=dataset.feature_names)\n",
    "#viz.view()\n",
    "# this opens the visualization in a new window. If you want to display the output inside the notebook use:\n",
    "viz\n",
    "# if you want to store the output in a file use:\n",
    "#viz.save(\"dtreeviz.svg\")\n",
    "# instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability\n",
    "\n",
    "If you want to visualize (explain) the decision path for one prediction, you can also use dtreeviz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 6.8s\n",
    "\n",
    "import numpy as np\n",
    "inx=np.random.randint(0, len(X_test))\n",
    "\n",
    "sample = X_test[inx,:] # random sample from training\n",
    "\n",
    "viz = dtreeviz(regressor2, X, y,\n",
    "                target_name=\"target\",\n",
    "                feature_names=dataset.feature_names,\n",
    "                X=sample)\n",
    "#viz.view()\n",
    "viz\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[inx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For bigger graphs you just show the decision path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 10.4s\n",
    "\n",
    "viz = dtreeviz(regressor3, X, y,\n",
    "                target_name=\"target\",\n",
    "                feature_names=dataset.feature_names,\n",
    "                X=sample,\n",
    "                show_just_path=True)\n",
    "#viz.view()\n",
    "viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option to explain the prediction for big trees is this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 0.1s\n",
    "\n",
    "from dtreeviz.trees import explain_prediction_path\n",
    "\n",
    "print(explain_prediction_path(regressor3, sample, feature_names=dataset.feature_names, explanation_type=\"plain_english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning\n",
    "\n",
    "### Cost Complexity Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A smart way of pruning is to use cost complexity pruning. This method is based on the idea that a tree with a lot of nodes is more likely to overfit than a tree with few nodes. Therefore, we can prune the tree by removing nodes that are not important for the prediction. The cost complexity pruning method uses a parameter $\\alpha$ to determine how many nodes to remove. It basically is a tradeoff between having a tree with many nodes that has a small total MSE, vs. a tree with fewer nodes but greater total MSE. The following code shows how to use the cost complexity pruning method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the alphas that change the Decision Tree to be \"cut down\" and we record the worsening of the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 0.8s\n",
    "\n",
    "path = regressor1.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "ccp_alphas[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plot the MSE for each $\\alpha$.\n",
    "\n",
    "\n",
    "\n",
    "## Is impurity gini- impurity or MSE ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# run time 0.4s\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax.set_xlabel(\"effective alpha\")\n",
    "ax.set_ylabel(\"total impurity of leaves\")\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now train a Decision Tree for each $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 0.2s\n",
    "\n",
    "regressors = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    regressor = DecisionTreeRegressor(min_samples_leaf=20, random_state=0, ccp_alpha=ccp_alpha)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    regressors.append(regressor)\n",
    "print(\n",
    "    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
    "        regressors[-1].tree_.node_count, ccp_alphas[-1]\n",
    "    ),\n",
    ")\n",
    "if regressors[-1].tree_.node_count == 1:\n",
    "    print(\"Removing last node.\")\n",
    "    regressors = regressors[:-1]\n",
    "    ccp_alphas = ccp_alphas[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 0.5s\n",
    "\n",
    "node_counts = [regressor.tree_.node_count for regressor in regressors]\n",
    "depth = [regressor.tree_.max_depth for regressor in regressors]\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "ax[0].plot(ccp_alphas, node_counts, marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax[0].set_xlabel(\"alpha\")\n",
    "ax[0].set_ylabel(\"number of nodes\")\n",
    "ax[0].set_title(\"Number of nodes vs alpha\")\n",
    "ax[1].plot(ccp_alphas, depth, marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax[1].set_xlabel(\"alpha\")\n",
    "ax[1].set_ylabel(\"depth of tree\")\n",
    "ax[1].set_title(\"Depth vs alpha\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a way to get all the scores for each tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 0.6s\n",
    "\n",
    "train_scores = [regressor.score(X_train, y_train) for regressor in regressors]\n",
    "val_scores = [regressor.score(X_val, y_val) for regressor in regressors]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"Accuracy vs alpha for training and validation sets\")\n",
    "ax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, val_scores, marker=\"o\", label=\"val\", drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best tree is the one with the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 0.8s\n",
    "\n",
    "idx_max = np.argmax(val_scores)\n",
    "regressor_best = regressors[idx_max]\n",
    "print(\"Best alpha: {}\".format(ccp_alphas[idx_max]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 0.7s\n",
    "regressor_best.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 0.8s\n",
    "\n",
    "dot_data = tree.export_graphviz(regressor_best, feature_names=dataset.feature_names, out_file=None, filled=True, rounded=True, special_characters=True)\n",
    "graph = graphviz.Source(dot_data, format=\"png\") \n",
    "graph.render(\"decision_tree_regressor_best\")\n",
    "Image(\"decision_tree_regressor_best.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble methods: \n",
    "\n",
    "Experiment with at least two methods. Inspect the documentation of the different estimators. Note that you can use regressors as estimators within an ensemble that are themselves based on an ensemble. Below is an example for a (tiny) voting ensemble. Visualise your results to be able to discuss them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 1.2s\n",
    "\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "voting=VotingRegressor(estimators=[('lr', LinearRegression()) ,('dt', DecisionTreeRegressor()) ,   ('hr', HuberRegressor())])\n",
    "voting.fit(X_train, y_train)\n",
    "voting.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting!\n",
    "\n",
    "Experiment with an AdaBoostRegressor and interpret the results. \n",
    "\n",
    "## Öka mängden träd hjälper lite. Till en viss gräns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 0.2s\n",
    "\n",
    "#https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_regression.html#sphx-glr-auto-examples-ensemble-plot-adaboost-regression-py\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "number_of_trees = 10\n",
    "\n",
    "boosting = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=number_of_trees, random_state=0)\n",
    "boosting.fit(X_train, y_train)\n",
    "boosting.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 1m 13s to 3m\n",
    "\n",
    "fig, ax = plt.subplots(5,2)\n",
    "for i, axi in enumerate(ax.flat):\n",
    " \n",
    "    axi.set_title(\"Tree {}\".format(i))\n",
    "    tree.plot_tree(boosting.estimators_[i], ax=axi, feature_names=dataset.feature_names, filled=True, rounded=True)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "Experiment with different parameters for the RF-Regressor. Test at least two different parameter sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "number_of_trees = 10\n",
    "forest = RandomForestRegressor(n_estimators=number_of_trees, random_state=0)\n",
    "forest.fit(X_train, y_train)\n",
    "forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for treeid in range(number_of_trees):\n",
    "    dot_data = tree.export_graphviz(forest.estimators_[treeid], feature_names=dataset.feature_names, out_file=None, filled=True, rounded=True, special_characters=True)\n",
    "    graph = graphviz.Source(dot_data, format=\"png\") \n",
    "    graph.render(\"forest_treeid\"+str(treeid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time 1m 23s to 3m\n",
    "\n",
    "fig, ax = plt.subplots(5,2)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.set_title(\"Tree {}\".format(i))\n",
    "    tree.plot_tree(forest.estimators_[i], ax=axi, feature_names=dataset.feature_names, filled=True, rounded=True)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "Implement your own version of a regression tree by using the provided code skeleton which you can find in ID3_reg.py. If you prefer to implement your own tree entirely, this is fine, but you should be confident in handling the implementation of the recursion properly. Please note: the actual task description (which evaluations to provide and discuss) is given in Canvas, as is a check list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install ordered_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretizing the data\n",
    "\n",
    "To make experiments with categorical data, you need to discretize the data (this goes both for the 'california' and the 'diabetes' cases). In order to have the entire data set \"as is\" for the binning, you can prepare a binning rule on the original data (X, if you continue to work with the data from part 1), that you then apply to your train and test data sets. To make sure that you do not miss any possible attribute values, use the entire set (X) again when providing the categorical values (here bin indices) to the ID3 tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import numpy as np\n",
    "\n",
    "# you might want to try different numbers of bins\n",
    "# inspect the data set description in part 1 to find suitable numbers \n",
    "if test_case == 'california':\n",
    "    bins = [2,2,2,2,2,2,2,2]\n",
    "elif test_case == 'diabetes':\n",
    "    bins = [2,2,2,2,2,2,2,2,2,2]\n",
    "\n",
    "# here you can test to use different strategies, see the KBinsDiscretizer documentation\n",
    "binner = KBinsDiscretizer(n_bins=bins, encode='ordinal', strategy='kmeans')\n",
    "binning_rule = binner.fit(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and evaluating an ID3-based Regression Tree\n",
    "\n",
    "The following cells provide some framework for creating / testing your own, ID3-based, regressor. To see that your tree is constructed correctly, some prints are provided (essentially from the lecture) within the handout directory, that are created with the \"ConceptData\" from the lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ID3_reg\n",
    "import ConceptDataRegr as cd\n",
    "import graphviz\n",
    "# For testing that you get the correct output from your own implementation, use the \"ConceptData\" data set:\n",
    "test_case = 'concept'\n",
    "\n",
    "if test_case == 'concept' :\n",
    "    attributes, binned_X_train, y_train, binned_X_test, y_test = cd.ConceptDataRegr().get_data()\n",
    "    binned_X_val = []\n",
    "    y_val = []\n",
    "     \n",
    "else :\n",
    "    # if running on \"real\" data, you now need to use the binning_rule you computed above \n",
    "    binned_X = binning_rule.transform(X).astype(int)\n",
    "    binned_X_train = binning_rule.transform(X_train).astype(int)\n",
    "    binned_X_val = binning_rule.transform(X_val).astype(int)\n",
    "    binned_X_test = binning_rule.transform(X_test).astype(int)\n",
    "\n",
    "    attributes = {}\n",
    "    i = 0\n",
    "    for attr in dataset.feature_names :\n",
    "        attributes[attr] = set(binned_X[:,i])\n",
    "        i+=1\n",
    "\n",
    "print(attributes)\n",
    "\n",
    "# Now, set up the tree (inspect the ID3_reg class!)    \n",
    "id3 = ID3_reg.ID3RegressionTreePredictor()\n",
    "\n",
    "# visualising in the \"bubble\" format from the lecture\n",
    "myTree = id3.fit(binned_X_train, y_train, attributes)\n",
    "dot_data = id3.makeDotData().source\n",
    "graph = graphviz.Source(dot_data, format=\"pdf\")\n",
    "graph.render(test_case+\"_bubbles\")\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = id3.predict(binned_X_val)\n",
    "print(predicted)\n",
    "\n",
    "# As of now, the ID3_reg class does only provide a stubb of a score-method - please implement one according to\n",
    "# the description of DecisionTreeRegressor.score() for easier comparison with the scikit-learn trees!\n",
    "id3.score(binned_X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing to squares if you want a tree that looks more like those from part 1 ;-)\n",
    "dot_data_pretty='digraph Tree {\\n'+\\\n",
    "    'node [shape=box'+\\\n",
    "    ', style=\"rounded\", color=\"black\"'+\\\n",
    "    ', fontname=\"helvetica\"] ;\\n'+\\\n",
    "    'graph [ranksep=equally, splines=polyline] ;\\n'+\\\n",
    "    'edge [fontname=\"helvetica\"] ;\\n'+\\\n",
    "    dot_data[9:]\n",
    "\n",
    "graph = graphviz.Source(dot_data_pretty, format=\"png\")\n",
    "graph.render(test_case+\"_pretty\")\n",
    "#graph.view()\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "63f08e0be87bb1ec22e3a665002567369c2bb78585d8d1135c35fb08381ea5a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
